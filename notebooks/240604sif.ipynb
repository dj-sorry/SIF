{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yegor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/yegor/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from sif_src.loader import download_ms_marco, load_data, save_processed_data\n",
    "from sif_src.preprocess_data import preprocess_text\n",
    "\n",
    "#download Marco\n",
    "\n",
    "\n",
    "#data['processed_text'] = data['text'].apply(preprocess_text)\n",
    "#save_processed_data(data, 'data/processed/ms_marco_processed.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/v1.1 to /Users/yegor/.cache/huggingface/datasets/microsoft___parquet/v1.1-3b5a967d10b189e3/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d059305ff884f568e2bc03e36084c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2709838197b6484784c3aec55f6df9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ad91a6eef947cabcb07c274c191c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/82326 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959a43c1fbee42fa867f5278237edfcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10047 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc11d11e8b7c48599d415740a452290b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/9650 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NonMatchingSplitsSizesError",
     "evalue": "[{'expected': SplitInfo(name='validation', num_bytes=42665198, num_examples=10047, shard_lengths=None, dataset_name=None), 'recorded': SplitInfo(name='validation', num_bytes=456580881, num_examples=111140, shard_lengths=None, dataset_name='parquet')}, {'expected': SplitInfo(name='train', num_bytes=350516260, num_examples=82326, shard_lengths=None, dataset_name=None), 'recorded': SplitInfo(name='train', num_bytes=3814528639, num_examples=891057, shard_lengths=[122326, 125533, 125533, 125533, 125533, 125533, 115533, 25533], dataset_name='parquet')}, {'expected': SplitInfo(name='test', num_bytes=40977580, num_examples=9650, shard_lengths=None, dataset_name=None), 'recorded': SplitInfo(name='test', num_bytes=446819375, num_examples=110742, shard_lengths=None, dataset_name='parquet')}]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNonMatchingSplitsSizesError\u001b[0m               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m download_ms_marco(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/raw_data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/学校/S24/CompLing/2406_sif/SIF/notebooks/../sif_src/loader.py:6\u001b[0m, in \u001b[0;36mdownload_ms_marco\u001b[0;34m(data_dir)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_ms_marco\u001b[39m(data_dir):\n\u001b[0;32m----> 6\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/ms_marco\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/conda/anaconda3/lib/python3.11/site-packages/datasets/load.py:1797\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1794\u001b[0m try_from_hf_gcs \u001b[38;5;241m=\u001b[39m path \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   1796\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 1797\u001b[0m builder_instance\u001b[38;5;241m.\u001b[39mdownload_and_prepare(\n\u001b[1;32m   1798\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[1;32m   1799\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[1;32m   1800\u001b[0m     verification_mode\u001b[38;5;241m=\u001b[39mverification_mode,\n\u001b[1;32m   1801\u001b[0m     try_from_hf_gcs\u001b[38;5;241m=\u001b[39mtry_from_hf_gcs,\n\u001b[1;32m   1802\u001b[0m     num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[1;32m   1803\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m   1804\u001b[0m )\n\u001b[1;32m   1806\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   1807\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1808\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   1809\u001b[0m )\n",
      "File \u001b[0;32m~/conda/anaconda3/lib/python3.11/site-packages/datasets/builder.py:890\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    889\u001b[0m         prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m--> 890\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_and_prepare(\n\u001b[1;32m    891\u001b[0m         dl_manager\u001b[38;5;241m=\u001b[39mdl_manager,\n\u001b[1;32m    892\u001b[0m         verification_mode\u001b[38;5;241m=\u001b[39mverification_mode,\n\u001b[1;32m    893\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_split_kwargs,\n\u001b[1;32m    894\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdownload_and_prepare_kwargs,\n\u001b[1;32m    895\u001b[0m     )\n\u001b[1;32m    896\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/conda/anaconda3/lib/python3.11/site-packages/datasets/builder.py:1003\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m     dl_manager\u001b[38;5;241m.\u001b[39mmanage_extracted_files()\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS \u001b[38;5;129;01mor\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS:\n\u001b[0;32m-> 1003\u001b[0m     verify_splits(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits, split_dict)\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;66;03m# Update the info object with the splits.\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits \u001b[38;5;241m=\u001b[39m split_dict\n",
      "File \u001b[0;32m~/conda/anaconda3/lib/python3.11/site-packages/datasets/utils/info_utils.py:100\u001b[0m, in \u001b[0;36mverify_splits\u001b[0;34m(expected_splits, recorded_splits)\u001b[0m\n\u001b[1;32m     94\u001b[0m bad_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     95\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected\u001b[39m\u001b[38;5;124m\"\u001b[39m: expected_splits[name], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecorded\u001b[39m\u001b[38;5;124m\"\u001b[39m: recorded_splits[name]}\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m expected_splits\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m expected_splits[name]\u001b[38;5;241m.\u001b[39mnum_examples \u001b[38;5;241m!=\u001b[39m recorded_splits[name]\u001b[38;5;241m.\u001b[39mnum_examples\n\u001b[1;32m     98\u001b[0m ]\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(bad_splits) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NonMatchingSplitsSizesError(\u001b[38;5;28mstr\u001b[39m(bad_splits))\n\u001b[1;32m    101\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll the splits matched successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNonMatchingSplitsSizesError\u001b[0m: [{'expected': SplitInfo(name='validation', num_bytes=42665198, num_examples=10047, shard_lengths=None, dataset_name=None), 'recorded': SplitInfo(name='validation', num_bytes=456580881, num_examples=111140, shard_lengths=None, dataset_name='parquet')}, {'expected': SplitInfo(name='train', num_bytes=350516260, num_examples=82326, shard_lengths=None, dataset_name=None), 'recorded': SplitInfo(name='train', num_bytes=3814528639, num_examples=891057, shard_lengths=[122326, 125533, 125533, 125533, 125533, 125533, 115533, 25533], dataset_name='parquet')}, {'expected': SplitInfo(name='test', num_bytes=40977580, num_examples=9650, shard_lengths=None, dataset_name=None), 'recorded': SplitInfo(name='test', num_bytes=446819375, num_examples=110742, shard_lengths=None, dataset_name='parquet')}]"
     ]
    }
   ],
   "source": [
    "data = download_ms_marco('../data/raw_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sif_src.utils import load_word_vectors\n",
    "from sif_src.sif import compute_word_frequencies, compute_sif_weights, compute_sif_embeddings, remove_pc\n",
    "\n",
    "\n",
    "word_vectors = load_word_vectors('path_to_word_vectors.txt')\n",
    "\n",
    "corpus = data['processed_text'].tolist()\n",
    "\n",
    "word_freq = compute_word_frequencies(corpus)\n",
    "sif_weights = compute_sif_weights(word_freq)\n",
    "\n",
    "embeddings = compute_sif_embeddings(corpus, word_vectors, sif_weights)\n",
    "\n",
    "final_embeddings = remove_pc(embeddings)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
